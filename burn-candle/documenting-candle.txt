Documenting the Candle backend creation

In burn, cargo new burn-candle
Remove main.rs

cargo.toml
Update basic infos, depend on derive-new for derive use
Depend on burn-tch for backend ref
Depend on autodiff for autogen tests
Depend on burn-tensor for tensors

Make lib.rs
Make the files element.rs, tensor.rs, backend.rs
Make ops dir with mod.rs

Write backend.rs
Make device, only cpu for now
#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum CandleDevice {
    /// The CPU device.
    Cpu,
    // TODO other devices
}
impl Default for CandleDevice {
    fn default() -> Self {
        Self::Cpu
    }
}

Make backend. By implementing Backend we will have to fill stuff	
/// The Candle backend.
#[derive(Clone, Copy, Default, Debug)]
pub struct CandleBackend<E> {
    _e: E,
}
impl<E> Backend for CandleBackend<E> {}

type Device = CandleDevice;
precisions: pas sûr pour l'instant. mettre E pour float, f32 pour full, et i64 pour int, comme pytorchs

name: "candle".to_string()
seed: leave to todo as it is not immediately obvious

ad_enabled: false pour l'instant car on va le wrapper dans ADBackend. Cependant on pourrait avoir leur autodiff à eux pour comparer les performances dans le futur

for element.rs
copy from tch
in cargo.toml add half for half precisions
replace tch by candle

Because of external crate candle, we need to import candle

candle-core because that's where they call kernels
https://crates.io/crates/candle-core
V 0.1.1

Device:
les deux from
Avoir cpu et Cuda

CandleElement: WithDType (pas sûr)
ne supporte pas u32 ou i64... à voir (un dans element un dans candle)

tensor.rs: take candle_core::tensor
Storage = ???
from_existing (inspiré de tch) -> ne pas oublier, quand storage sera figured out

element: u32 ajouté dans burn-tensor